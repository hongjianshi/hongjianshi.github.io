---
---

@article {shi2025existence,
  title   = {Existence of direct density ratio estimators},
  author  = {Banzato, Erika and Drton, Mathias and Saraf-Poor, Kian and Shi, Hongjian},
  note    = {arXiv:2502.12738},
  abbr    = {Preprint},
  year    = {2025},
 abstract = {Many two-sample problems call for a comparison of two distributions from an exponential family. Density ratio estimation methods provide ways to solve such problems through direct estimation of the differences in natural parameters. The term direct indicates that one avoids estimating both marginal distributions. In this context, we consider the Kullback--Leibler Importance Estimation Procedure (KLIEP), which has been the subject of recent work on differential networks. Our main result shows that the existence of the KLIEP estimator is characterized by whether the average sufficient statistic for one sample belongs to the convex hull of the set of all sufficient statistics for data points in the second sample. For high-dimensional problems it is customary to regularize the KLIEP loss by adding the product of a tuning parameter and a norm of the vector of parameter differences. We show that the existence of the regularized KLIEP estimator requires the tuning parameter to be no less than the dual norm-based distance between the average sufficient statistic and the convex hull. The implications of these existence issues are explored in applications to differential network analysis.},
      arxiv = {2502.12738},
 bibtex_show = {true},
}

@article {shi2024universal,
  title   = {On universal inference in {G}aussian mixture models},
  author  = {Shi, Hongjian and Drton, Mathias},
  note    = {arXiv:2407.19361},
  abbr    = {Preprint},
  year    = {2024},
 abstract = {Recent work on game-theoretic statistics and safe anytime-valid inference (SAVI) provides new tools for statistical inference without assuming any regularity conditions. In particular, the framework of universal inference proposed by Wasserman, Ramdas, and Balakrishnan (2020) offers new solutions by modifying the likelihood ratio test in a data-splitting scheme. In this paper, we study the performance of the resulting split likelihood ratio test under Gaussian mixture models, which are canonical examples for models in which classical regularity conditions fail to hold. We first establish that under the null hypothesis, the split likelihood ratio statistic is asymptotically normal with increasing mean and variance. Moreover, contradicting the usual belief that the flexibility of SAVI and universal methods comes at the price of a significant loss of power, we are able to prove that universal inference surprisingly achieves the same detection rate $(n^{âˆ’1} \log \log n)^{1/2}$ as the classical likelihood ratio test.},
      arxiv = {2407.19361},
 bibtex_show = {true},
}